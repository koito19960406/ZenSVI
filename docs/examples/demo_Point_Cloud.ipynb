{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dba7e11",
   "metadata": {},
   "source": [
    "# Depth Estimation & Point Cloud Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e54c1",
   "metadata": {},
   "source": [
    "The tutorial demonstrates how to use ZenSVI to estimatie depth information from street view imagery, and further integrate the depth and color information to reconstruct point cloud.  \n",
    "Contributer: [Zicheng Fan](https://github.com/fzc961020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74d5a0",
   "metadata": {},
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eea71b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade zensvi\n",
    "#pip install img2vec_pytorch\n",
    "#pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0aa556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Weights file already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zicheng/.conda/envs/torch_cuda_118/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current notebook's directory (docs/examples) dynamically\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "\n",
    "# Construct the path to the src folder relative to the notebook location\n",
    "src_path = os.path.normpath(os.path.join(notebook_dir, '../../src/'))\n",
    "\n",
    "# Add the src folder to sys.path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Now import your package\n",
    "from zensvi.transform import PointCloudProcessor\n",
    "from zensvi.cv import DepthEstimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05d407",
   "metadata": {},
   "source": [
    "## Download the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce05a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, hf_hub_download\n",
    "\n",
    "\n",
    "def download_folder(repo_id, repo_type, folder_path, local_dir):\n",
    "    \"\"\"\n",
    "    Download an entire folder from a huggingface dataset repository.\n",
    "    repo_id : string\n",
    "        The ID of the repository (e.g., 'username/repo_name').\n",
    "    repo_type : string\n",
    "        Type of the repo, dataset or model.\n",
    "    folder_path : string\n",
    "        The path to the folder within the repository.\n",
    "    local_dir : string\n",
    "        Local folder to download the data. This mimics git behaviour\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    # list all files in the repo, keep the ones within folder_path\n",
    "    all_files = api.list_repo_files(repo_id, repo_type=repo_type)\n",
    "    files_list = [f for f in all_files if f.startswith(folder_path)]\n",
    "\n",
    "    # download each of those files\n",
    "    for file_path in files_list:\n",
    "        hf_hub_download(repo_id=repo_id, repo_type=repo_type,\n",
    "                        filename=file_path, local_dir=local_dir)\n",
    "\n",
    "\n",
    "# Download test dataset for the example\n",
    "repo_id = \"NUS-UAL/zensvi_test_data\" # the test dataset repo\n",
    "repo_type = \"dataset\" # required by the API when the repo is a dataset\n",
    "folder_path = \"input/depth_point_cloud/\" # the specific data\n",
    "local_dir = \"zensvi_example_data/\" # the local folder in your computer where it will be downloaded\n",
    "\n",
    "# By default, huggingface download them to the .cache/huggingface folder\n",
    "download_folder(repo_id, repo_type, folder_path, local_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec0a1d",
   "metadata": {},
   "source": [
    "## Depth Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8dd93",
   "metadata": {},
   "source": [
    "[Depth Anything V2](https://github.com/DepthAnything/Depth-Anything-V2) is applied in ZenSVI to infer both absolute and relative depth based on Street View Imagery (SVI).\n",
    "Here we attempt the absolute depth inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f144299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating depth: 100%|██████████| 2/2 [00:11<00:00,  5.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from zensvi.cv import DepthEstimator\n",
    "\n",
    "depth_estimator = DepthEstimator(\n",
    "    device=\"cpu\",  # device to use (either \"cpu\" or \"gpu\")\n",
    "    task=\"absolute\" # task to perform (either \"relative\" or \"absolute\")\n",
    ")\n",
    "\n",
    "dir_input = \"zensvi_example_data/input/depth_point_cloud/images/color\"\n",
    "dir_image_output = \"zensvi_example_data/input/depth_point_cloud/images/depth\" # estimated depth map\n",
    "depth_estimator.estimate_depth(\n",
    "    dir_input,\n",
    "    dir_image_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac8d8e",
   "metadata": {},
   "source": [
    "## Point Cloud Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ffb30",
   "metadata": {},
   "source": [
    "### Part 1: Define the PointCloudProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a7f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the class PointCloudProcessor is defined as in the previous block or imported successfully\n",
    "# Initialize the processor with paths to your image (color and depth) folders\n",
    "processor = PointCloudProcessor(\n",
    "    image_folder='zensvi_example_data/input/depth_point_cloud/images/color',\n",
    "    depth_folder='zensvi_example_data/input/depth_point_cloud/images/depth'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7664196",
   "metadata": {},
   "source": [
    "We can visualize a color image and corresponding depth image from the two folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62716035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "color_path = Path('zensvi_example_data/input/depth_point_cloud/images/color/VSsVjWlr4orKerabFRy-dQ.jpg')\n",
    "depth_path = Path('zensvi_example_data/input/depth_point_cloud/images/depth/VSsVjWlr4orKerabFRy-dQ.tiff')\n",
    "\n",
    "# Read color image\n",
    "raw_img = cv2.imread(str(color_path))\n",
    "# Read depth image\n",
    "depth_img = cv2.imread(str(depth_path), cv2.IMREAD_UNCHANGED)\n",
    "depth = depth_img.astype(np.float32)\n",
    "# Normalize depth map to 0-1 range\n",
    "depth_normalized = (depth - depth.min()) / (depth.max() - depth.min())\n",
    "\n",
    "# Create vertical layout subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Display original image\n",
    "ax1.imshow(cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title('Original Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Display depth map\n",
    "ax2.imshow(depth_normalized, cmap='jet')\n",
    "ax2.set_title('Depth Map')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print depth map information\n",
    "print(f\"Depth map dimensions: {depth.shape}\")\n",
    "print(f\"Depth value range: [{depth.min()}, {depth.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Reconstruct point clouds from single panorama and absolute depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Convert single group of images to point cloud\n",
    "pcd = processor.convert_to_point_cloud(depth, raw_img, depth_max=None, use_absolute_depth=True)\n",
    "\n",
    "# Visualize point cloud using plotly\n",
    "processor.visualize_point_cloud(\n",
    "    pcd,\n",
    "    marker_size=1,\n",
    "    opacity=0.8,\n",
    "    camera_eye=dict(x=0, y=0, z=-1),\n",
    "    camera_up=dict(x=0, y=-1, z=0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf054a7",
   "metadata": {},
   "source": [
    "### Part 3: Reconstruct from multiple panorama with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d27014",
   "metadata": {},
   "source": [
    "ZenSVI also support convert multiple SVI inputs to point clouds, within the help of indexing file.  \n",
    "An example dataframe is shown as below.  \n",
    "The image id is the only necessary attributes for indexing color and depth image when generating single point cloud.\n",
    "Besides the image id, possible metadata includes: image angle ('heading'), and the real-world coordinates of image ('x_proj','y_proj'), depending on the availability. They are useful in processing multiple images, and aligning point clouds generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da215a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>id</th>\n",
       "      <th>heading</th>\n",
       "      <th>geometry</th>\n",
       "      <th>y_proj</th>\n",
       "      <th>x_proj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>40.773640</td>\n",
       "      <td>-73.954823</td>\n",
       "      <td>Y2y7An1aRCeA5Y4nW7ITrg</td>\n",
       "      <td>3.627108</td>\n",
       "      <td>POINT (-8232613.214232705 4979010.676803163)</td>\n",
       "      <td>-8232613.214</td>\n",
       "      <td>4979010.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>40.775753</td>\n",
       "      <td>-73.956686</td>\n",
       "      <td>VSsVjWlr4orKerabFRy-dQ</td>\n",
       "      <td>5.209303</td>\n",
       "      <td>POINT (-8232820.629621736 4979321.30902424)</td>\n",
       "      <td>-8232820.630</td>\n",
       "      <td>4979321.309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  year  month        lat        lon                      id  \\\n",
       "0           0  2018      8  40.773640 -73.954823  Y2y7An1aRCeA5Y4nW7ITrg   \n",
       "1           1  2019      5  40.775753 -73.956686  VSsVjWlr4orKerabFRy-dQ   \n",
       "\n",
       "    heading                                      geometry       y_proj  \\\n",
       "0  3.627108  POINT (-8232613.214232705 4979010.676803163) -8232613.214   \n",
       "1  5.209303   POINT (-8232820.629621736 4979321.30902424) -8232820.630   \n",
       "\n",
       "        x_proj  \n",
       "0  4979010.677  \n",
       "1  4979321.309  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input the images\n",
    "import pandas as pd\n",
    "data = pd.read_csv('zensvi_example_data/input/depth_point_cloud/meta_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc571ded",
   "metadata": {},
   "source": [
    "We can load images as dictionary of array according to the indexing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1bead8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Y2y7An1aRCeA5Y4nW7ITrg': {'depth': array([[35, 35, 35, ..., 32, 32, 31],\n",
       "         [36, 35, 35, ..., 32, 33, 32],\n",
       "         [36, 35, 34, ..., 32, 33, 33],\n",
       "         ...,\n",
       "         [ 6,  6,  6, ...,  6,  6,  6],\n",
       "         [ 6,  6,  6, ...,  6,  6,  6],\n",
       "         [ 6,  6,  6, ...,  6,  6,  6]], dtype=uint8),\n",
       "  'color': array([[[132, 171, 230],\n",
       "          [132, 171, 230],\n",
       "          [132, 171, 230],\n",
       "          ...,\n",
       "          [130, 173, 226],\n",
       "          [130, 173, 226],\n",
       "          [130, 173, 226]],\n",
       "  \n",
       "         [[132, 171, 230],\n",
       "          [132, 171, 230],\n",
       "          [132, 171, 230],\n",
       "          ...,\n",
       "          [130, 173, 226],\n",
       "          [130, 173, 226],\n",
       "          [130, 173, 226]],\n",
       "  \n",
       "         [[131, 171, 230],\n",
       "          [131, 171, 230],\n",
       "          [131, 171, 230],\n",
       "          ...,\n",
       "          [130, 173, 228],\n",
       "          [130, 173, 228],\n",
       "          [130, 173, 228]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[103, 107, 118],\n",
       "          [106, 110, 121],\n",
       "          [111, 115, 126],\n",
       "          ...,\n",
       "          [ 98, 107, 116],\n",
       "          [ 98, 107, 116],\n",
       "          [ 97, 106, 115]],\n",
       "  \n",
       "         [[111, 113, 125],\n",
       "          [112, 114, 126],\n",
       "          [114, 116, 128],\n",
       "          ...,\n",
       "          [106, 113, 121],\n",
       "          [105, 112, 120],\n",
       "          [104, 111, 119]],\n",
       "  \n",
       "         [[133, 135, 147],\n",
       "          [133, 135, 147],\n",
       "          [133, 135, 147],\n",
       "          ...,\n",
       "          [136, 143, 151],\n",
       "          [136, 143, 151],\n",
       "          [136, 143, 151]]], dtype=uint8)},\n",
       " 'VSsVjWlr4orKerabFRy-dQ': {'depth': array([[30, 30, 30, ..., 30, 30, 29],\n",
       "         [29, 29, 29, ..., 29, 29, 28],\n",
       "         [29, 29, 29, ..., 29, 29, 29],\n",
       "         ...,\n",
       "         [ 6,  6,  6, ...,  6,  6,  6],\n",
       "         [ 6,  6,  6, ...,  6,  6,  6],\n",
       "         [ 6,  6,  6, ...,  6,  6,  6]], dtype=uint8),\n",
       "  'color': array([[[249, 255, 251],\n",
       "          [249, 255, 251],\n",
       "          [249, 255, 251],\n",
       "          ...,\n",
       "          [254, 255, 255],\n",
       "          [254, 255, 255],\n",
       "          [254, 255, 255]],\n",
       "  \n",
       "         [[245, 255, 255],\n",
       "          [243, 255, 255],\n",
       "          [245, 255, 255],\n",
       "          ...,\n",
       "          [248, 255, 255],\n",
       "          [247, 255, 255],\n",
       "          [247, 255, 255]],\n",
       "  \n",
       "         [[223, 247, 255],\n",
       "          [221, 248, 255],\n",
       "          [223, 247, 255],\n",
       "          ...,\n",
       "          [229, 249, 255],\n",
       "          [228, 248, 255],\n",
       "          [228, 248, 255]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[ 75,  60,  57],\n",
       "          [ 78,  63,  60],\n",
       "          [ 79,  64,  61],\n",
       "          ...,\n",
       "          [ 65,  49,  49],\n",
       "          [ 66,  50,  50],\n",
       "          [ 66,  50,  50]],\n",
       "  \n",
       "         [[ 80,  65,  62],\n",
       "          [ 83,  68,  65],\n",
       "          [ 83,  68,  65],\n",
       "          ...,\n",
       "          [ 66,  50,  50],\n",
       "          [ 67,  51,  51],\n",
       "          [ 68,  52,  52]],\n",
       "  \n",
       "         [[ 87,  72,  69],\n",
       "          [ 89,  74,  71],\n",
       "          [ 90,  75,  72],\n",
       "          ...,\n",
       "          [ 67,  51,  51],\n",
       "          [ 69,  53,  53],\n",
       "          [ 69,  53,  53]]], dtype=uint8)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all the images based on the datafrome\n",
    "images = processor._load_images(data)\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate point clouds from specific image in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "258db409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 131072 points."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_id = 'Y2y7An1aRCeA5Y4nW7ITrg'\n",
    "\n",
    "depth_img = images[image_id][\"depth\"]\n",
    "depth = depth_img.astype(np.float32)\n",
    "color_img = images[image_id][\"color\"]\n",
    "\n",
    "pcd = processor.convert_to_point_cloud(depth_img, color_img, depth_max=None, use_absolute_depth=True)\n",
    "pcd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc640b",
   "metadata": {},
   "source": [
    "We can also process multiple images with loop and apply some point cloud processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf9d470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PointCloud with 131072 points., PointCloud with 131072 points.]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate point clouds from all the images in the dataframe\n",
    "point_clouds = processor.process_multiple_images(data,depth_max=None, use_absolute_depth=True)\n",
    "point_clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806fd6c6",
   "metadata": {},
   "source": [
    "With the point clouds generated, we can further modify and clean them. \n",
    "The processing steps include:\n",
    "   - relocate the point clouds to their real-world coordinates;\n",
    "   - align the point clouds according to the 'heading' information stored with SVI;\n",
    "   - crop the point clouds based on a self-defined 3D bounding box (to remove unnecessary part)\n",
    "    \n",
    "The part will be improved with more functions and more explict control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "535c2d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PointCloud with 108589 points., PointCloud with 119811 points.]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, transform the point clouds\n",
    "transformed_clouds = []\n",
    "for i, pcd in enumerate(point_clouds):\n",
    "    origin_x = data.at[i, 'x_proj'] \n",
    "    origin_y = data.at[i, 'y_proj'] \n",
    "    angle = data.at[i, 'heading']\n",
    "    box_extent = [100, 100, 100]  # Example box dimensions\n",
    "    box_center = [origin_x, origin_y, 1]  # Example box center\n",
    "    transformed_pcd = processor.transform_point_cloud(pcd, origin_x, origin_y, angle, box_extent, box_center) # crop and transform the point clouds with the parameters\n",
    "    transformed_clouds.append(transformed_pcd)\n",
    "transformed_clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc94cd4",
   "metadata": {},
   "source": [
    "Similarly, we can visualize the transformed point clouds in 3d with plotly library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05150d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the second transformed point cloud (for demonstration)\n",
    "# Visualize point cloud using plotly\n",
    "processor.visualize_point_cloud(\n",
    "    transformed_clouds[1],\n",
    "    marker_size=1,\n",
    "    opacity=0.8,\n",
    "    camera_eye=dict(x=0, y=0, z=-1),\n",
    "    camera_up=dict(x=0, y=-1, z=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01132d5c",
   "metadata": {},
   "source": [
    "### Part 5: Save point cloud in different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe8fad",
   "metadata": {},
   "source": [
    "The generated point clouds can be saved to local in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "194f4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds = processor.process_multiple_images(data,depth_max=None,use_absolute_depth=True, output_dir='zensvi_example_data/output/pointclouds', save_format=\"pcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26936837",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds = processor.process_multiple_images(data,depth_max=None,use_absolute_depth=True, output_dir='zensvi_example_data/output/pointclouds', save_format=\"ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a972f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds = processor.process_multiple_images(data,depth_max=None,use_absolute_depth=True, output_dir='zensvi_example_data/output/pointclouds', save_format='npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f027d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds = processor.process_multiple_images(data,depth_max=None,use_absolute_depth=True, output_dir='zensvi_example_data/output/pointclouds', save_format='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Point Cloud Reconstruction via VGGT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an intial attempt, we have integrate Visual Geometry Grounded Transformer (VGGT) into ZenSVI and are exploring a different way to reconstruct street scenes with SVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VGGT Processor Initialization Started ===\n",
      "VGGT Path: /data/zicheng/zensvi_update/20250616_updates/ZenSVI/src/zensvi/transform/vggt\n",
      "Added VGGT path to sys.path\n",
      "Utils Path: /data/zicheng/zensvi_update/20250616_updates/ZenSVI/src/zensvi/transform/vggt/vggt/utils\n",
      "Added Utils path to sys.path\n",
      "Using Device: cuda\n",
      "Data Type: torch.bfloat16\n",
      "Loading VGGT model with local cache: /data/zicheng/zensvi_update/20250616_updates/ZenSVI/models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VGGT Processor Initialization Completed ===\n",
      "Warning: Found images with different shapes: {(476, 518), (504, 518), (462, 518), (490, 518), (448, 518), (518, 518)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "# Add necessary paths\n",
    "# Construct the path to the src folder relative to the notebook location\n",
    "src_path = os.path.normpath(os.path.join(notebook_dir, '../../src/zensvi/transform'))\n",
    "\n",
    "# Add the src folder to sys.path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "\n",
    "from image_to_pointcloud_vggt import VGGTProcessor\n",
    "\n",
    "# Initialize VGGT processor\n",
    "vggt_processor = VGGTProcessor(\n",
    ")\n",
    "\n",
    "# Set input image path\n",
    "# Automatically get all images from the image folder\n",
    "image_folder = \"zensvi_example_data/input/depth_point_cloud/images/perspective\"\n",
    "image_extensions = [\".png\", \".jpg\", \".jpeg\"]\n",
    "image_names = []\n",
    "\n",
    "for ext in image_extensions:\n",
    "    image_names.extend(list(Path(image_folder).glob(f\"*{ext}\")))\n",
    "    image_names.extend(list(Path(image_folder).glob(f\"*{ext.upper()}\")))\n",
    "\n",
    "\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "# Process images and generate point cloud\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=dtype):\n",
    "        # Use VGGT processor to generate depth map and point cloud\n",
    "        predictions = vggt_processor.process_images(image_names)\n",
    "        points_centered, colors_flat, conf_flat, cam_to_world = vggt_processor.generate_point_cloud(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vggt_processor.visualize_point_cloud(\n",
    "    points_centered, colors_flat,\n",
    "    marker_size=1,\n",
    "    opacity=0.8,\n",
    "    camera_eye=dict(x=0, y=0, z=-1),\n",
    "    camera_up=dict(x=0, y=-1, z=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
